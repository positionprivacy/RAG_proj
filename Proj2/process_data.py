import os
from document_loader import DocumentLoader
from text_splitter import TextSplitter
from vector_store import VectorStore
from file_classifier import FileClassifier # [æ–°å¢]
from tqdm import tqdm
from typing import Optional
from config import DATA_DIR, CHUNK_SIZE, CHUNK_OVERLAP, VECTOR_DB_PATH
def process_single_file(
    file_path: str, 
    forced_course_name: Optional[str] = None
) -> str:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶çš„æ ¸å¿ƒé€»è¾‘ï¼ˆä¾› Web ç«¯è°ƒç”¨ï¼‰
    
    å‚æ•°:
        file_path: æ–‡ä»¶è·¯å¾„
        forced_course_name: 
            - None: è®© AI è‡ªåŠ¨åˆ¤æ–­
            - "xxx": å¼ºåˆ¶å½’ç±»ä¸º xxx è¯¾ç¨‹
            
    è¿”å›:
        å¤„ç†ç»“æœæ¶ˆæ¯
    """
    # 1. åˆå§‹åŒ–ç»„ä»¶
    loader = DocumentLoader(data_dir=os.path.dirname(file_path))
    splitter = TextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    vector_store = VectorStore(db_path=VECTOR_DB_PATH)
    classifier = FileClassifier()

    filename = os.path.basename(file_path)
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if vector_store.is_file_processed(filename):
        return f"æ–‡ä»¶ {filename} å·²å­˜åœ¨äºçŸ¥è¯†åº“ä¸­ï¼Œå·²è·³è¿‡ã€‚"

    # 2. åŠ è½½æ–‡æ¡£
    docs = loader.load_document(file_path)
    if not docs:
        return f"æ–‡ä»¶ {filename} åŠ è½½å¤±è´¥æˆ–å†…å®¹ä¸ºç©ºã€‚"

    # 3. ç¡®å®šè¯¾ç¨‹åç§°
    final_course_name = "æœªçŸ¥è¯¾ç¨‹"
    
    if forced_course_name:
        # A. ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®š/åˆ›å»º
        final_course_name = forced_course_name
        print(f"  [Manual] å¼ºåˆ¶å½’ç±»ä¸º: {final_course_name}")
    else:
        # B. AI è‡ªåŠ¨åˆ¤æ–­
        preview_text = docs[0]['content']
        # è·å–ç°æœ‰è¯¾ç¨‹ä½œä¸ºè®°å¿†
        current_courses = vector_store.get_all_courses()
        final_course_name = classifier.determine_course(
            filename, preview_text, list(current_courses)
        )
        print(f"  [AI Auto] è‡ªåŠ¨å½’ç±»ä¸º: {final_course_name}")

    # 4. å†™å…¥å…ƒæ•°æ®
    for doc in docs:
        doc['course_name'] = final_course_name

    # 5. åˆ‡åˆ†
    chunks = splitter.split_documents(docs)

    # 6. å­˜å…¥
    vector_store.add_documents(chunks)
    
    return f"æˆåŠŸå¤„ç† {filename}ï¼Œå½’å…¥åˆ†åŒºï¼š[{final_course_name}]ï¼Œç”Ÿæˆ {len(chunks)} ä¸ªçŸ¥è¯†å—ã€‚"
def main():
    if not os.path.exists(DATA_DIR):
        print(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
        return

    # 1. åˆå§‹åŒ–ç»„ä»¶
    loader = DocumentLoader(data_dir=DATA_DIR)
    splitter = TextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    vector_store = VectorStore(db_path=VECTOR_DB_PATH)
    classifier = FileClassifier() # [æ–°å¢]

    # [å…³é”®] è·å–å½“å‰å·²æœ‰çš„è¯¾ç¨‹åˆ—è¡¨ï¼Œä½œä¸º"è®°å¿†"
    # ä½¿ç”¨ set æ–¹ä¾¿å¿«é€Ÿå»é‡
    current_courses = set(vector_store.get_all_courses())
    print(f"ğŸ” å½“å‰å‘é‡åº“ä¸­å·²å­˜åœ¨çš„è¯¾ç¨‹åˆ†åŒº: {current_courses if current_courses else 'æ—  (å†·å¯åŠ¨)'}")

    # 2. æ‰«ææ–‡ä»¶
    supported_formats = [".pdf", ".pptx", ".docx", ".txt", ".py"]
    files_to_process = []

    print("æ­£åœ¨æ‰«ææ–°æ–‡ä»¶...")
    for root, dirs, files in os.walk(DATA_DIR):
        for file in files:
            if file.startswith("~$") or file.startswith("."):
                continue
            ext = os.path.splitext(file)[1].lower()
            if ext in supported_formats:
                file_path = os.path.join(root, file)
                
                if vector_store.is_file_processed(file):
                    print(f"  [è·³è¿‡] å·²å­˜åœ¨: {file}")
                else:
                    print(f"  [æ–°å¢] å¾…å¤„ç†: {file}")
                    files_to_process.append(file_path)

    if not files_to_process:
        print("\næ‰€æœ‰æ–‡ä»¶å‡å·²å¤„ç†ï¼Œæ— éœ€æ›´æ–°ã€‚")
        return

    # 3. é€ä¸ªå¤„ç†æ–°æ–‡ä»¶ (åŠ è½½ -> åˆ†ç±» -> æ ‡è®°)
    print(f"\nå¼€å§‹å¤„ç† {len(files_to_process)} ä¸ªæ–°æ–‡ä»¶...")
    
    new_documents = []
    
    # æˆ‘ä»¬é€ä¸ªå¤„ç†ï¼Œä»¥ä¾¿å®æ—¶æ›´æ–° current_courses
    for file_path in tqdm(files_to_process, desc="åŠ è½½å¹¶åˆ†ç±»"):
        # A. åŠ è½½æ–‡æ¡£
        docs = loader.load_document(file_path)
        if not docs: continue
        
        # B. æ™ºèƒ½åˆ†ç±»
        filename = os.path.basename(file_path)
        preview_text = docs[0]['content'] # å–ç¬¬ä¸€é¡µ/ç¬¬ä¸€å—å†…å®¹é¢„è§ˆ
        
        # è°ƒç”¨åˆ†ç±»å™¨ï¼Œä¼ å…¥å½“å‰çš„è®°å¿†
        course_name = classifier.determine_course(
            filename=filename, 
            content_preview=preview_text, 
            existing_courses=list(current_courses)
        )
        
        # C. æ›´æ–°è®°å¿† (è¿™æ ·è¿™ä¸€æ‰¹æ¬¡åç»­çš„æ–‡ä»¶å°±èƒ½çœ‹åˆ°è¿™ä¸ªæ–°åå­—)
        if course_name not in current_courses:
            print(f"\n  âœ¨ æ–°å»ºåˆ†åŒº: [{course_name}] (æ–‡ä»¶: {filename})")
            current_courses.add(course_name)
        else:
            # å¯ä»¥åœ¨ tqdm è¿›åº¦æ¡å¤–æ‰“å°ï¼Œé¿å…åˆ·å±
            pass 

        # D. å°†è¯¾ç¨‹åæ‰“å…¥ Metadata
        for doc in docs:
            doc['course_name'] = course_name
            
        new_documents.extend(docs)

    if not new_documents:
        print("æœªæå–åˆ°æœ‰æ•ˆå†…å®¹")
        return

    # 4. åˆ‡åˆ†æ–‡æ¡£ (æ­¤æ—¶ docs é‡Œå·²ç»æœ‰äº† course_name)
    chunks = splitter.split_documents(new_documents)

    # 5. å­˜å…¥å‘é‡æ•°æ®åº“
    vector_store.add_documents(chunks)
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼å½“å‰åº“ä¸­æ€»è®¡ {vector_store.get_collection_count()} æ¡æ•°æ®ã€‚")
    print(f"ğŸ“š å½“å‰æ‰€æœ‰è¯¾ç¨‹åˆ†åŒº: {list(current_courses)}")

if __name__ == "__main__":
    main()