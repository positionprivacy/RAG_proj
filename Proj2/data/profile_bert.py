#!/usr/bin/env python3
"""
BERTæ¨¡å‹CUDA Kernel Profilingè„šæœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰
ç”¨äºåˆ†æTransformeræ¨¡å‹ä¸­çš„çƒ­ç‚¹ç®—å­
åŒ…å«è¯¦ç»†çš„kernelä¿¡æ¯ã€å†…å­˜åˆ†æå’Œç®—å­æ˜ å°„
"""

import torch
import torch.nn as nn
from torch.profiler import profile, record_function, ProfilerActivity
import time
import json
import os
from collections import defaultdict
import re

# åˆ›å»ºä¸€ä¸ªç®€å•çš„Transformer Blockç”¨äºæµ‹è¯•
class SimpleTransformerBlock(nn.Module):
    def __init__(self, hidden_size=768, num_heads=12, intermediate_size=3072):
        super().__init__()
        self.attention = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True)
        self.layernorm1 = nn.LayerNorm(hidden_size)
        self.layernorm2 = nn.LayerNorm(hidden_size)
        self.ffn = nn.Sequential(
            nn.Linear(hidden_size, intermediate_size),
            nn.GELU(),
            nn.Linear(intermediate_size, hidden_size)
        )
    
    def forward(self, x):
        # Self-attention with residual
        attn_out, _ = self.attention(x, x, x)
        x = self.layernorm1(x + attn_out)
        
        # FFN with residual
        ffn_out = self.ffn(x)
        x = self.layernorm2(x + ffn_out)
        
        return x


def profile_transformer(batch_size=8, seq_len=128, hidden_size=768, use_real_bert=False):
    """
    Profiling Transformeræ¨¡å‹
    
    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        seq_len: åºåˆ—é•¿åº¦
        hidden_size: éšè—å±‚ç»´åº¦
        use_real_bert: æ˜¯å¦ä½¿ç”¨çœŸå®çš„BERTæ¨¡å‹ï¼ˆéœ€è¦transformersåº“ï¼‰
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    if use_real_bert:
        try:
            from transformers import BertModel
            model = BertModel.from_pretrained('bert-base-uncased').to(device)
            print("ä½¿ç”¨BERT-baseæ¨¡å‹")
        except:
            print("æœªå®‰è£…transformersåº“ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å‹")
            model = SimpleTransformerBlock(hidden_size).to(device)
    else:
        model = SimpleTransformerBlock(hidden_size).to(device)
    
    model.eval()
    
    # åˆ›å»ºè¾“å…¥æ•°æ®
    if use_real_bert:
        input_ids = torch.randint(0, 30522, (batch_size, seq_len), device=device)
        inputs = {'input_ids': input_ids}
    else:
        inputs = torch.randn(batch_size, seq_len, hidden_size, device=device)
    
    # é¢„çƒ­
    print("é¢„çƒ­GPU...")
    with torch.no_grad():
        for _ in range(10):
            if use_real_bert:
                _ = model(**inputs)
            else:
                _ = model(inputs)
    torch.cuda.synchronize()
    
    # Profiling
    print(f"\nå¼€å§‹Profiling (batch_size={batch_size}, seq_len={seq_len})...")
    
    with profile(
        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
        record_shapes=True,
        profile_memory=True,
        with_stack=True,
        with_flops=True
    ) as prof:
        with record_function("transformer_forward"):
            with torch.no_grad():
                for _ in range(20):  # å¤šæ¬¡è¿è¡Œè·å¾—æ›´ç¨³å®šçš„ç»Ÿè®¡
                    if use_real_bert:
                        _ = model(**inputs)
                    else:
                        _ = model(inputs)
    
    torch.cuda.synchronize()
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*100)
    print("CUDA Kernel çƒ­ç‚¹åˆ†æ (æŒ‰CUDAæ—¶é—´æ’åº)")
    print("="*100)
    print(prof.key_averages().table(
        sort_by="cuda_time_total",
        row_limit=30,
        max_name_column_width=80
    ))
    
    print("\n" + "="*100)
    print("CUDA Kernel è°ƒç”¨æ¬¡æ•°ç»Ÿè®¡")
    print("="*100)
    print(prof.key_averages().table(
        sort_by="self_cuda_time_total",
        row_limit=30,
        max_name_column_width=80
    ))
    
    # å¯¼å‡ºChrome traceç”¨äºå¯è§†åŒ–
    trace_file = f"bert_trace_bs{batch_size}_seq{seq_len}.json"
    prof.export_chrome_trace(trace_file)
    print(f"\nChrome traceå·²å¯¼å‡ºåˆ°: {trace_file}")
    print("åœ¨Chromeæµè§ˆå™¨ä¸­æ‰“å¼€ chrome://tracing å¹¶åŠ è½½è¯¥æ–‡ä»¶è¿›è¡Œå¯è§†åŒ–åˆ†æ")
    
    # ============ å¢å¼ºçš„è¯¦ç»†åˆ†æ ============
    analysis_results = analyze_profiling_results(prof, batch_size, seq_len, hidden_size)
    
    # æ‰“å°è¯¦ç»†åˆ†æ
    print_detailed_analysis(analysis_results)
    
    # ä¿å­˜è¯¦ç»†ç»Ÿè®¡æ•°æ®
    stats_file = f"profiling_stats_bs{batch_size}_seq{seq_len}.json"
    with open(stats_file, 'w') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False)
    print(f"\nè¯¦ç»†ç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°: {stats_file}")
    
    # ç”Ÿæˆç®—å­è°ƒç ”æŠ¥å‘Š
    report_file = f"kernel_analysis_report_bs{batch_size}_seq{seq_len}.md"
    generate_analysis_report(analysis_results, report_file)
    print(f"ç®—å­åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    return prof


def analyze_profiling_results(prof, batch_size, seq_len, hidden_size):
    """
    è¯¦ç»†åˆ†æprofilingç»“æœ
    
    Returns:
        dict: åŒ…å«è¯¦ç»†åˆ†æç»“æœçš„å­—å…¸
    """
    all_ops = []
    cuda_kernels = []
    aten_ops = {}
    op_categories = defaultdict(list)
    total_cuda_time = 0
    total_cpu_time = 0
    
    # éå†æ‰€æœ‰äº‹ä»¶
    for evt in prof.key_averages():
        name = evt.key
        cuda_time_ms = evt.cuda_time_total / 1000.0
        cpu_time_ms = evt.cpu_time_total / 1000.0
        
        total_cuda_time += cuda_time_ms
        total_cpu_time += cpu_time_ms
        
        # åŸºæœ¬ä¿¡æ¯
        op_info = {
            'name': name,
            'cuda_time_total_ms': cuda_time_ms,
            'cuda_time_avg_ms': evt.cuda_time / 1000.0 if evt.count > 0 else 0,
            'cpu_time_total_ms': cpu_time_ms,
            'cpu_time_avg_ms': evt.cpu_time / 1000.0 if evt.count > 0 else 0,
            'count': evt.count,
            'self_cuda_time_ms': evt.self_cuda_time_total / 1000.0,
            'self_cpu_time_ms': evt.self_cpu_time_total / 1000.0,
        }
        
        # æ·»åŠ å†…å­˜ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(evt, 'cuda_memory_usage'):
            op_info['cuda_memory_usage'] = evt.cuda_memory_usage
        
        # æ·»åŠ shapeä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(evt, 'input_shapes') and evt.input_shapes:
            op_info['input_shapes'] = str(evt.input_shapes)
        
        # æ·»åŠ FLOPsä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(evt, 'flops') and evt.flops > 0:
            op_info['flops'] = evt.flops
        
        all_ops.append(op_info)
        
        # åˆ†ç±»ï¼šCUDA kernel vs ATenç®—å­
        if 'void ' in name or '::' in name and 'aten::' not in name:
            cuda_kernels.append(op_info)
        elif 'aten::' in name:
            aten_ops[name] = op_info
            # æŒ‰ç®—å­ç±»å‹åˆ†ç±»
            category = categorize_operator(name)
            op_categories[category].append(op_info)
    
    # æŒ‰æ—¶é—´æ’åº
    all_ops.sort(key=lambda x: x['cuda_time_total_ms'], reverse=True)
    cuda_kernels.sort(key=lambda x: x['cuda_time_total_ms'], reverse=True)
    
    # ç»Ÿè®¡å„ç±»åˆ«çš„å æ¯”
    category_stats = {}
    for category, ops in op_categories.items():
        total_time = sum(op['cuda_time_total_ms'] for op in ops)
        category_stats[category] = {
            'total_time_ms': total_time,
            'percentage': (total_time / total_cuda_time * 100) if total_cuda_time > 0 else 0,
            'op_count': len(ops),
            'call_count': sum(op['count'] for op in ops)
        }
    
    # è¯†åˆ«Topç®—å­å¹¶æ˜ å°„åˆ°native_functions
    top_aten_ops = sorted(aten_ops.values(), key=lambda x: x['cuda_time_total_ms'], reverse=True)[:10]
    top_ops_with_mapping = []
    
    for op in top_aten_ops:
        op_mapping = map_to_native_function(op['name'])
        top_ops_with_mapping.append({
            **op,
            'native_function': op_mapping['function'],
            'potential_cuda_file': op_mapping['cuda_file'],
            'category': categorize_operator(op['name'])
        })
    
    return {
        'config': {
            'batch_size': batch_size,
            'seq_len': seq_len,
            'hidden_size': hidden_size,
            'device': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',
            'cuda_version': torch.version.cuda,
            'pytorch_version': torch.__version__
        },
        'summary': {
            'total_cuda_time_ms': total_cuda_time,
            'total_cpu_time_ms': total_cpu_time,
            'total_operators': len(all_ops),
            'aten_operators': len(aten_ops),
            'cuda_kernels': len(cuda_kernels)
        },
        'top_operators': all_ops[:30],
        'top_aten_operators': top_ops_with_mapping,
        'top_cuda_kernels': cuda_kernels[:20],
        'category_statistics': category_stats,
        'detailed_category_breakdown': {
            category: sorted(ops, key=lambda x: x['cuda_time_total_ms'], reverse=True)[:5]
            for category, ops in op_categories.items()
        }
    }


def categorize_operator(op_name):
    """å°†ç®—å­åˆ†ç±»"""
    name_lower = op_name.lower()
    
    categories = {
        'Matrix Operations': ['mm', 'matmul', 'bmm', 'addmm', 'baddbmm', 'gemm'],
        'Attention': ['attention', 'scaled_dot_product'],
        'Normalization': ['layer_norm', 'layernorm', 'batch_norm', 'group_norm'],
        'Activation': ['gelu', 'relu', 'silu', 'sigmoid', 'tanh', 'softmax'],
        'Embedding': ['embedding', 'gather'],
        'Elementwise': ['add', 'mul', 'div', 'sub', 'pow'],
        'Reduction': ['sum', 'mean', 'max', 'min'],
        'Memory': ['copy', 'clone', 'contiguous', 'view', 'reshape', 'transpose'],
        'Other': []
    }
    
    for category, keywords in categories.items():
        if any(keyword in name_lower for keyword in keywords):
            return category
    
    return 'Other'


def map_to_native_function(op_name):
    """
    å°†ATenç®—å­æ˜ å°„åˆ°native_functions.yamlä¸­çš„å‡½æ•°å’ŒCUDAå®ç°
    """
    # æå–ç®—å­åç§°ï¼ˆå»æ‰aten::å‰ç¼€ï¼‰
    if 'aten::' in op_name:
        func_name = op_name.split('aten::')[1].split('.')[0].split('(')[0]
    else:
        func_name = op_name.split('::')[-1].split('.')[0].split('(')[0]
    
    # å¸¸è§ç®—å­çš„CUDAå®ç°æ–‡ä»¶æ˜ å°„
    cuda_file_mapping = {
        'softmax': 'aten/src/ATen/native/cuda/SoftMax.cu',
        'layer_norm': 'aten/src/ATen/native/cuda/layer_norm_kernel.cu',
        'batch_norm': 'aten/src/ATen/native/cuda/Normalization.cu',
        'addmm': 'aten/src/ATen/native/cuda/Blas.cpp',
        'mm': 'aten/src/ATen/native/cuda/Blas.cpp',
        'bmm': 'aten/src/ATen/native/cuda/Blas.cpp',
        'matmul': 'aten/src/ATen/native/cuda/Blas.cpp',
        'gelu': 'aten/src/ATen/native/cuda/Activation.cu',
        'relu': 'aten/src/ATen/native/cuda/Activation.cu',
        'silu': 'aten/src/ATen/native/cuda/Activation.cu',
        'embedding': 'aten/src/ATen/native/cuda/Embedding.cu',
        'dropout': 'aten/src/ATen/native/cuda/Dropout.cu',
        'linear': 'aten/src/ATen/native/cuda/Linear.cu',
        'add': 'aten/src/ATen/native/cuda/BinaryOps.cu',
        'mul': 'aten/src/ATen/native/cuda/BinaryOps.cu',
    }
    
    cuda_file = cuda_file_mapping.get(func_name.lower(), f'aten/src/ATen/native/cuda/{func_name}.cu')
    
    return {
        'function': func_name,
        'cuda_file': cuda_file,
        'native_function_entry': f'{func_name} in native_functions.yaml'
    }


def print_detailed_analysis(results):
    """æ‰“å°è¯¦ç»†çš„åˆ†æç»“æœ"""
    print("\n" + "="*100)
    print("ğŸ“Š PROFILING è¯¦ç»†åˆ†ææŠ¥å‘Š")
    print("="*100)
    
    # 1. é…ç½®ä¿¡æ¯
    print("\n1ï¸âƒ£ é…ç½®ä¿¡æ¯:")
    print("-" * 100)
    config = results['config']
    print(f"  Batch Size: {config['batch_size']}")
    print(f"  Sequence Length: {config['seq_len']}")
    print(f"  Hidden Size: {config['hidden_size']}")
    print(f"  Device: {config['device']}")
    print(f"  CUDA Version: {config['cuda_version']}")
    print(f"  PyTorch Version: {config['pytorch_version']}")
    
    # 2. æ€»ä½“ç»Ÿè®¡
    print("\n2ï¸âƒ£ æ€»ä½“ç»Ÿè®¡:")
    print("-" * 100)
    summary = results['summary']
    print(f"  æ€»CUDAæ—¶é—´: {summary['total_cuda_time_ms']:.2f} ms")
    print(f"  æ€»CPUæ—¶é—´: {summary['total_cpu_time_ms']:.2f} ms")
    print(f"  ATenç®—å­æ•°é‡: {summary['aten_operators']}")
    print(f"  CUDA Kernelæ•°é‡: {summary['cuda_kernels']}")
    print(f"  æ€»ç®—å­æ•°é‡: {summary['total_operators']}")
    
    # 3. ç®—å­ç±»åˆ«å æ¯”
    print("\n3ï¸âƒ£ ç®—å­ç±»åˆ«å æ¯”:")
    print("-" * 100)
    print(f"{'ç±»åˆ«':<25} {'æ€»æ—¶é—´(ms)':<15} {'å æ¯”(%)':<10} {'ç®—å­æ•°':<10} {'è°ƒç”¨æ¬¡æ•°'}")
    print("-" * 100)
    for category, stats in sorted(results['category_statistics'].items(), 
                                   key=lambda x: x[1]['total_time_ms'], reverse=True):
        print(f"{category:<25} {stats['total_time_ms']:<15.2f} {stats['percentage']:<10.1f} "
              f"{stats['op_count']:<10} {stats['call_count']}")
    
    # 4. Top 10 ATenç®—å­ï¼ˆå¸¦æ˜ å°„ä¿¡æ¯ï¼‰
    print("\n4ï¸âƒ£ Top 10 ATenç®—å­ï¼ˆå«native_functionsæ˜ å°„ï¼‰:")
    print("-" * 100)
    print(f"{'ç®—å­åç§°':<40} {'æ€»æ—¶é—´(ms)':<12} {'è°ƒç”¨æ¬¡æ•°':<10} {'ç±»åˆ«':<20}")
    print(f"{'Nativeå‡½æ•°':<40} {'CUDAæ–‡ä»¶':<50}")
    print("-" * 100)
    
    for op in results['top_aten_operators']:
        print(f"{op['name']:<40} {op['cuda_time_total_ms']:<12.3f} {op['count']:<10} {op['category']:<20}")
        print(f"  â””â”€ {op['native_function']:<38} {op['potential_cuda_file']:<50}")
        print()
    
    # 5. Top 15 CUDA Kernels
    print("\n5ï¸âƒ£ Top 15 åº•å±‚CUDA Kernels:")
    print("-" * 100)
    print(f"{'Kernelåç§°':<80} {'æ€»æ—¶é—´(ms)':<12} {'è°ƒç”¨æ¬¡æ•°'}")
    print("-" * 100)
    for kernel in results['top_cuda_kernels'][:15]:
        # æˆªæ–­è¿‡é•¿çš„kernelåç§°
        kernel_name = kernel['name']
        if len(kernel_name) > 80:
            kernel_name = kernel_name[:77] + "..."
        print(f"{kernel_name:<80} {kernel['cuda_time_total_ms']:<12.3f} {kernel['count']}")
    
    # 6. å„ç±»åˆ«Topç®—å­
    print("\n6ï¸âƒ£ å„ç±»åˆ«Topç®—å­è¯¦æƒ…:")
    print("-" * 100)
    for category, ops in results['detailed_category_breakdown'].items():
        if not ops:
            continue
        print(f"\nã€{category}ã€‘")
        for i, op in enumerate(ops[:3], 1):
            print(f"  {i}. {op['name']}")
            print(f"     æ—¶é—´: {op['cuda_time_total_ms']:.3f}ms (å¹³å‡: {op['cuda_time_avg_ms']:.4f}ms)")
            print(f"     è°ƒç”¨: {op['count']}æ¬¡")


def generate_analysis_report(results, output_file):
    """ç”ŸæˆMarkdownæ ¼å¼çš„ç®—å­åˆ†ææŠ¥å‘Š"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# BERTæ¨¡å‹CUDAç®—å­Profilingåˆ†ææŠ¥å‘Š\n\n")
        
        # é…ç½®ä¿¡æ¯
        f.write("## 1. å®éªŒé…ç½®\n\n")
        config = results['config']
        f.write(f"- **Batch Size**: {config['batch_size']}\n")
        f.write(f"- **Sequence Length**: {config['seq_len']}\n")
        f.write(f"- **Hidden Size**: {config['hidden_size']}\n")
        f.write(f"- **GPUè®¾å¤‡**: {config['device']}\n")
        f.write(f"- **CUDAç‰ˆæœ¬**: {config['cuda_version']}\n")
        f.write(f"- **PyTorchç‰ˆæœ¬**: {config['pytorch_version']}\n\n")
        
        # æ€»ä½“ç»Ÿè®¡
        f.write("## 2. æ€§èƒ½æ€»è§ˆ\n\n")
        summary = results['summary']
        f.write(f"- **æ€»CUDAæ—¶é—´**: {summary['total_cuda_time_ms']:.2f} ms\n")
        f.write(f"- **æ€»CPUæ—¶é—´**: {summary['total_cpu_time_ms']:.2f} ms\n")
        f.write(f"- **ATenç®—å­æ•°é‡**: {summary['aten_operators']}\n")
        f.write(f"- **CUDA Kernelæ•°é‡**: {summary['cuda_kernels']}\n\n")
        
        # ç®—å­ç±»åˆ«ç»Ÿè®¡
        f.write("## 3. ç®—å­ç±»åˆ«åˆ†å¸ƒ\n\n")
        f.write("| ç±»åˆ« | æ€»æ—¶é—´(ms) | å æ¯”(%) | ç®—å­æ•° | è°ƒç”¨æ¬¡æ•° |\n")
        f.write("|------|-----------|---------|--------|----------|\n")
        for category, stats in sorted(results['category_statistics'].items(),
                                       key=lambda x: x[1]['total_time_ms'], reverse=True):
            f.write(f"| {category} | {stats['total_time_ms']:.2f} | "
                   f"{stats['percentage']:.1f} | {stats['op_count']} | {stats['call_count']} |\n")
        f.write("\n")
        
        # Topç®—å­è¯¦ç»†ä¿¡æ¯
        f.write("## 4. Top 10 å…³é”®ç®—å­è¯¦ç»†åˆ†æ\n\n")
        for i, op in enumerate(results['top_aten_operators'], 1):
            f.write(f"### 4.{i} {op['name']}\n\n")
            f.write(f"**æ€§èƒ½æŒ‡æ ‡:**\n")
            f.write(f"- æ€»CUDAæ—¶é—´: {op['cuda_time_total_ms']:.3f} ms\n")
            f.write(f"- å¹³å‡CUDAæ—¶é—´: {op['cuda_time_avg_ms']:.4f} ms\n")
            f.write(f"- è°ƒç”¨æ¬¡æ•°: {op['count']}\n")
            f.write(f"- ç®—å­ç±»åˆ«: {op['category']}\n\n")
            
            f.write(f"**æºç ä¿¡æ¯:**\n")
            f.write(f"- Nativeå‡½æ•°: `{op['native_function']}`\n")
            f.write(f"- CUDAå®ç°æ–‡ä»¶: `{op['potential_cuda_file']}`\n")
            f.write(f"- native_functions.yamlå£°æ˜: `{op['native_function']}`\n\n")
            
            f.write(f"**è°ƒç ”è¦ç‚¹:**\n")
            category = op['category']
            if category == 'Matrix Operations':
                f.write("- åˆ†æcuBLASåº“è°ƒç”¨\n")
                f.write("- ç ”ç©¶çŸ©é˜µåˆ†å—ç­–ç•¥\n")
                f.write("- è€ƒå¯Ÿshared memoryä½¿ç”¨\n")
                f.write("- å¹¶è¡Œç»´åº¦: block/thread tiling\n")
            elif category == 'Activation' and 'softmax' in op['name'].lower():
                f.write("- åˆ†æwarp-level reduction\n")
                f.write("- ç ”ç©¶æ•°å€¼ç¨³å®šæ€§å¤„ç†ï¼ˆmax subtractionï¼‰\n")
                f.write("- è€ƒå¯Ÿonline softmaxç®—æ³•\n")
                f.write("- å¹¶è¡Œç»´åº¦: æ¯ä¸ªwarpå¤„ç†ä¸€è¡Œ\n")
            elif category == 'Normalization':
                f.write("- åˆ†æWelfordç®—æ³•å®ç°\n")
                f.write("- ç ”ç©¶ä¸¤é˜¶æ®µå½’çº¦ï¼ˆå‡å€¼å’Œæ–¹å·®ï¼‰\n")
                f.write("- è€ƒå¯Ÿæ•°å€¼ç¨³å®šæ€§\n")
                f.write("- å¹¶è¡Œç»´åº¦: æ²¿normalizationç»´åº¦å¹¶è¡Œ\n")
            elif 'gelu' in op['name'].lower():
                f.write("- åˆ†æGELUè¿‘ä¼¼æ–¹æ³•ï¼ˆtanh vs erfï¼‰\n")
                f.write("- ç ”ç©¶å‘é‡åŒ–å®ç°\n")
                f.write("- è€ƒå¯Ÿmemory coalescing\n")
                f.write("- å¹¶è¡Œç»´åº¦: elementwiseå¹¶è¡Œ\n")
            
            f.write("\n")
        
        # ä¼˜åŒ–å»ºè®®
        f.write("## 5. æ€§èƒ½ä¼˜åŒ–å»ºè®®\n\n")
        f.write("### 5.1 ä¼˜å…ˆçº§1 - é«˜å½±å“ç®—å­\n\n")
        
        top3 = results['top_aten_operators'][:3]
        for i, op in enumerate(top3, 1):
            percentage = (op['cuda_time_total_ms'] / results['summary']['total_cuda_time_ms']) * 100
            f.write(f"{i}. **{op['native_function']}** (å æ€»æ—¶é—´{percentage:.1f}%)\n")
            f.write(f"   - å½“å‰è€—æ—¶: {op['cuda_time_total_ms']:.2f}ms\n")
            f.write(f"   - ä¼˜åŒ–ç›®æ ‡: å‡å°‘10-30%æ‰§è¡Œæ—¶é—´\n\n")
        
        f.write("### 5.2 ç®—å­èåˆæœºä¼š\n\n")
        f.write("- Fused Attention (QKV projection + Attention)\n")
        f.write("- Fused FFN (Linear + GELU + Linear)\n")
        f.write("- Fused LayerNorm + Linear\n\n")
        
        # ä¸‹ä¸€æ­¥è¡ŒåŠ¨
        f.write("## 6. ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’\n\n")
        f.write("- [ ] æ·±å…¥åˆ†æTop 3ç®—å­çš„CUDAå®ç°æºç \n")
        f.write("- [ ] ä½¿ç”¨Nsight Computeè¿›è¡Œkernelçº§åˆ«çš„è¯¦ç»†åˆ†æ\n")
        f.write("- [ ] å®ç°ä¼˜åŒ–ç‰ˆæœ¬çš„å…³é”®ç®—å­\n")
        f.write("- [ ] è¿›è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•\n")
        f.write("- [ ] æ’°å†™è¯¦ç»†çš„ç®—å­åˆ†ææŠ¥å‘Š\n\n")
    
    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")


def benchmark_kernels():
    """
    å•ç‹¬benchmarkå…³é”®ç®—å­ï¼Œæµ‹è¯•ä¸åŒè¾“å…¥å¤§å°
    """
    print("\n" + "="*100)
    print("ğŸ”¬ å•ç‹¬Benchmarkå…³é”®ç®—å­")
    print("="*100)
    
    device = torch.device('cuda')
    
    # æµ‹è¯•é…ç½®
    configs = [
        {'batch_size': 8, 'seq_len': 128, 'hidden_size': 768},
        {'batch_size': 16, 'seq_len': 128, 'hidden_size': 768},
        {'batch_size': 8, 'seq_len': 256, 'hidden_size': 768},
    ]
    
    all_benchmark_results = []
    
    for config in configs:
        batch_size = config['batch_size']
        seq_len = config['seq_len']
        hidden_size = config['hidden_size']
        
        print(f"\n{'='*80}")
        print(f"é…ç½®: Batch={batch_size}, SeqLen={seq_len}, Hidden={hidden_size}")
        print(f"{'='*80}")
        
        benchmark_results = {
            'config': config,
            'kernels': {}
        }
        
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        
        # 1. Softmax (Attention scores)
        print("\n1ï¸âƒ£  Softmax (Attention)")
        x = torch.randn(batch_size, 12, seq_len, seq_len, device=device)
        torch.cuda.synchronize()
        
        for _ in range(10):
            _ = torch.softmax(x, dim=-1)
        
        start.record()
        for _ in range(100):
            _ = torch.softmax(x, dim=-1)
        end.record()
        torch.cuda.synchronize()
        
        softmax_time = start.elapsed_time(end) / 100
        print(f"   å¹³å‡æ—¶é—´: {softmax_time:.4f} ms")
        print(f"   è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"   å†…å­˜å¸¦å®½: {x.numel() * x.element_size() * 2 / (softmax_time / 1000) / 1e9:.2f} GB/s")
        benchmark_results['kernels']['softmax'] = softmax_time
        
        # 2. LayerNorm
        print("\n2ï¸âƒ£  LayerNorm")
        x = torch.randn(batch_size, seq_len, hidden_size, device=device)
        layer_norm = torch.nn.LayerNorm(hidden_size).to(device)
        
        for _ in range(10):
            _ = layer_norm(x)
        
        start.record()
        for _ in range(100):
            _ = layer_norm(x)
        end.record()
        torch.cuda.synchronize()
        
        ln_time = start.elapsed_time(end) / 100
        print(f"   å¹³å‡æ—¶é—´: {ln_time:.4f} ms")
        print(f"   è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"   å†…å­˜å¸¦å®½: {x.numel() * x.element_size() * 2 / (ln_time / 1000) / 1e9:.2f} GB/s")
        benchmark_results['kernels']['layernorm'] = ln_time
        
        # 3. MatMul (GEMM) - QKV projection
        print("\n3ï¸âƒ£  MatMul/GEMM (Linearå±‚)")
        A = torch.randn(batch_size * seq_len, hidden_size, device=device)
        B = torch.randn(hidden_size, hidden_size, device=device)
        
        for _ in range(10):
            _ = torch.matmul(A, B)
        
        start.record()
        for _ in range(100):
            _ = torch.matmul(A, B)
        end.record()
        torch.cuda.synchronize()
        
        mm_time = start.elapsed_time(end) / 100
        flops = 2 * A.shape[0] * A.shape[1] * B.shape[1]
        print(f"   å¹³å‡æ—¶é—´: {mm_time:.4f} ms")
        print(f"   è¾“å…¥å½¢çŠ¶: A={A.shape}, B={B.shape}")
        print(f"   FLOPs: {flops / 1e9:.2f} GFLOPs")
        print(f"   ååé‡: {flops / (mm_time / 1000) / 1e12:.2f} TFLOPs/s")
        benchmark_results['kernels']['matmul'] = mm_time
        
        # 4. BMM (Batch MatMul) - Attention QK^T
        print("\n4ï¸âƒ£  BMM (Attention QK^T)")
        Q = torch.randn(batch_size * 12, seq_len, 64, device=device)
        K = torch.randn(batch_size * 12, seq_len, 64, device=device)
        
        for _ in range(10):
            _ = torch.bmm(Q, K.transpose(1, 2))
        
        start.record()
        for _ in range(100):
            _ = torch.bmm(Q, K.transpose(1, 2))
        end.record()
        torch.cuda.synchronize()
        
        bmm_time = start.elapsed_time(end) / 100
        bmm_flops = 2 * Q.shape[0] * Q.shape[1] * Q.shape[1] * Q.shape[2]
        print(f"   å¹³å‡æ—¶é—´: {bmm_time:.4f} ms")
        print(f"   è¾“å…¥å½¢çŠ¶: Q={Q.shape}, K={K.shape}")
        print(f"   ååé‡: {bmm_flops / (bmm_time / 1000) / 1e12:.2f} TFLOPs/s")
        benchmark_results['kernels']['bmm'] = bmm_time
        
        # 5. GELU
        print("\n5ï¸âƒ£  GELUæ¿€æ´»å‡½æ•°")
        x = torch.randn(batch_size, seq_len, hidden_size * 4, device=device)
        gelu = torch.nn.GELU()
        
        for _ in range(10):
            _ = gelu(x)
        
        start.record()
        for _ in range(100):
            _ = gelu(x)
        end.record()
        torch.cuda.synchronize()
        
        gelu_time = start.elapsed_time(end) / 100
        print(f"   å¹³å‡æ—¶é—´: {gelu_time:.4f} ms")
        print(f"   è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"   å†…å­˜å¸¦å®½: {x.numel() * x.element_size() * 2 / (gelu_time / 1000) / 1e9:.2f} GB/s")
        benchmark_results['kernels']['gelu'] = gelu_time
        
        # 6. Dropout
        print("\n6ï¸âƒ£  Dropout")
        x = torch.randn(batch_size, seq_len, hidden_size, device=device)
        dropout = torch.nn.Dropout(0.1)
        
        for _ in range(10):
            _ = dropout(x)
        
        start.record()
        for _ in range(100):
            _ = dropout(x)
        end.record()
        torch.cuda.synchronize()
        
        dropout_time = start.elapsed_time(end) / 100
        print(f"   å¹³å‡æ—¶é—´: {dropout_time:.4f} ms")
        benchmark_results['kernels']['dropout'] = dropout_time
        
        all_benchmark_results.append(benchmark_results)
    
    # ä¿å­˜benchmarkç»“æœ
    benchmark_file = 'kernel_benchmark_results.json'
    with open(benchmark_file, 'w') as f:
        json.dump(all_benchmark_results, f, indent=2)
    print(f"\nâœ… Benchmarkç»“æœå·²ä¿å­˜åˆ°: {benchmark_file}")
    
    return all_benchmark_results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='BERTæ¨¡å‹CUDA Kernel Profilingï¼ˆå¢å¼ºç‰ˆï¼‰')
    parser.add_argument('--use-real-bert', action='store_true',
                        help='ä½¿ç”¨çœŸå®çš„BERTæ¨¡å‹ï¼ˆéœ€è¦transformersåº“ï¼‰')
    parser.add_argument('--batch-sizes', type=int, nargs='+', default=[1, 8, 16],
                        help='æ‰¹æ¬¡å¤§å°åˆ—è¡¨ï¼Œé»˜è®¤: 1 8 16')
    parser.add_argument('--seq-lens', type=int, nargs='+', default=[128, 256],
                        help='åºåˆ—é•¿åº¦åˆ—è¡¨ï¼Œé»˜è®¤: 128 256')
    parser.add_argument('--hidden-size', type=int, default=768,
                        help='éšè—å±‚å¤§å°ï¼Œé»˜è®¤: 768')
    parser.add_argument('--output-dir', type=str, default='./profiling_results',
                        help='è¾“å‡ºç›®å½•ï¼Œé»˜è®¤: ./profiling_results')
    parser.add_argument('--skip-benchmark', action='store_true',
                        help='è·³è¿‡å•ç‹¬çš„kernel benchmark')
    
    args = parser.parse_args()
    
    print("="*100)
    print("ğŸš€ Transformeræ¨¡å‹ CUDA Kernel Profiling (å¢å¼ºç‰ˆ)")
    print("="*100)
    
    if not torch.cuda.is_available():
        print("âŒ é”™è¯¯: æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡!")
        return
    
    print(f"\nğŸ“Š GPUä¿¡æ¯:")
    print(f"  è®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}")
    print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"  è®¡ç®—èƒ½åŠ›: {torch.cuda.get_device_capability(0)}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # åˆ‡æ¢åˆ°è¾“å‡ºç›®å½•
    original_dir = os.getcwd()
    os.chdir(args.output_dir)
    
    try:
        # ç”Ÿæˆæ‰€æœ‰é…ç½®ç»„åˆ
        configs = []
        for bs in args.batch_sizes:
            for seq_len in args.seq_lens:
                configs.append({
                    'batch_size': bs,
                    'seq_len': seq_len,
                    'hidden_size': args.hidden_size,
                    'use_real_bert': args.use_real_bert
                })
        
        print(f"\nğŸ“‹ å°†è¿è¡Œ {len(configs)} ä¸ªé…ç½®çš„profiling:")
        for i, config in enumerate(configs, 1):
            print(f"  {i}. Batch={config['batch_size']}, SeqLen={config['seq_len']}, "
                  f"Hidden={config['hidden_size']}, RealBERT={config['use_real_bert']}")
        
        # è¿è¡Œprofiling
        all_results = []
        for i, config in enumerate(configs, 1):
            print(f"\n{'='*100}")
            print(f"â³ [{i}/{len(configs)}] Profilingé…ç½®: "
                  f"Batch={config['batch_size']}, SeqLen={config['seq_len']}")
            print(f"{'='*100}")
            
            try:
                prof = profile_transformer(**config)
                all_results.append({
                    'config': config,
                    'success': True
                })
            except Exception as e:
                print(f"âŒ é…ç½® {config} æ‰§è¡Œå¤±è´¥: {e}")
                all_results.append({
                    'config': config,
                    'success': False,
                    'error': str(e)
                })
        
        # å•ç‹¬benchmarkï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not args.skip_benchmark and torch.cuda.is_available():
            benchmark_kernels()
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        print("\n" + "="*100)
        print("ğŸ“ˆ Profilingæ‰§è¡Œæ€»ç»“")
        print("="*100)
        
        successful = sum(1 for r in all_results if r['success'])
        print(f"\næˆåŠŸ: {successful}/{len(all_results)} ä¸ªé…ç½®")
        
        if successful > 0:
            print("\nç”Ÿæˆçš„æ–‡ä»¶:")
            for r in all_results:
                if r['success']:
                    config = r['config']
                    bs, sl = config['batch_size'], config['seq_len']
                    print(f"  - profiling_stats_bs{bs}_seq{sl}.json (è¯¦ç»†ç»Ÿè®¡)")
                    print(f"  - kernel_analysis_report_bs{bs}_seq{sl}.md (åˆ†ææŠ¥å‘Š)")
                    print(f"  - bert_trace_bs{bs}_seq{sl}.json (Chrome trace)")
        
        print("\n" + "="*100)
        print("âœ… Profilingå®Œæˆ!")
        print("="*100)
        print("\nğŸ“ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("  1. æŸ¥çœ‹ç”Ÿæˆçš„MarkdownæŠ¥å‘Šï¼Œè¯†åˆ«Top 3å…³é”®ç®—å­")
        print("  2. åœ¨Chromeæµè§ˆå™¨ä¸­æ‰“å¼€ chrome://tracing æŸ¥çœ‹traceæ–‡ä»¶")
        print("  3. æ ¹æ®æŠ¥å‘Šä¸­çš„æºç è·¯å¾„ï¼Œæ·±å…¥åˆ†æCUDAå®ç°")
        print("  4. å¼€å§‹æ’°å†™ç®—å­è°ƒç ”æ–‡æ¡£")
        print(f"\nğŸ“‚ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {os.path.abspath(args.output_dir)}")
        
    finally:
        # åˆ‡å›åŸç›®å½•
        os.chdir(original_dir)


if __name__ == '__main__':
    main()



